{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"yolov9_paper.pdf\") \n",
    "data = loader.load()  # entire PDF is loaded as a single Document\n",
    "# data\n",
    "\n",
    "# we can also take the data from website just we have to pass the website link by this we can create a company chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071e509d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc1032cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  96\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split data\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "print(\"Total number of documents: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03dbaa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-01T01:40:26+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-01T01:40:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov9_paper.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2'}, page_content='ditional layers to combine repeatedly fed input data, which\\nwill significantly increase the inference cost. In addition,\\nsince the input data layer to the output layer cannot have a\\ntoo deep path, this limitation will make it difficult to model\\nhigh-order semantic information during the training pro-\\ncess. As for masked modeling, its reconstruction loss some-\\ntimes conflicts with the target loss. In addition, most mask\\nmechanisms also produce incorrect associations with data.\\nFor the deep supervision mechanism, it will produce error\\naccumulation, and if the shallow supervision loses informa-\\ntion during the training process, the subsequent layers will\\nnot be able to retrieve the required information. The above\\nphenomenon will be more significant on difficult tasks and\\nsmall models.\\nTo address the above-mentioned issues, we propose a\\nnew concept, which is programmable gradient information\\n(PGI). The concept is to generate reliable gradients through')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c15dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05636945366859436,\n",
       " 0.004828543867915869,\n",
       " -0.07625909894704819,\n",
       " -0.023642510175704956,\n",
       " 0.053293220698833466]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "#Get an API key: \n",
    "# Head to https://ai.google.dev/gemini-api/docs/api-key to generate a Google AI API key. Paste in .env file\n",
    "\n",
    "# Embedding models: https://python.langchain.com/v0.1/docs/integrations/text_embedding/\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]\n",
    "#vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b378c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6060679",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What is new in yolov9?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "814c9d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ebde923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\n",
      "YOLOv9-M (S) 20.0 76.3 51.4 68.1 56.1 33.6 57.0 68.0\n",
      "Gold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\n",
      "Gold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\n",
      "Gold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\n",
      "PPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\n",
      "PPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\n",
      "RTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\n",
      "YOLOv9-C (S) 25.3 102.1 53.0 70.2 57.8 36.2 58.5 69.3\n",
      "DAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\n",
      "RT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\n",
      "RT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\n",
      "YOLOv9-E (S) 34.7 147.1 54.5 71.7 59.2 38.1 59.9 70.3\n",
      "YOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\n",
      "RT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\n",
      "Gold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\n",
      "Gold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\n",
      "Gold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\n",
      "RT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[5].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7939b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",temperature=0.3, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2eb80317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf7f7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9a546dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv9 introduces two main innovations: Programmable Gradient Information (PGI) and Generalized ELAN (GELAN). The network architecture replaces ELAN with GELAN, utilizing CSPNet blocks and RepConv as computational blocks. Additionally, YOLOv9 features a simplified downsampling module and an optimized anchor-free prediction head.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is new in YOLOv9?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_langchain1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
